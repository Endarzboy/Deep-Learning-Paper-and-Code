{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upEJK8UUDnyn"
   },
   "source": [
    "> ### EEE4423: Signal Processing Lab\n",
    "\n",
    "# LAB \\#12: Sequence to Sequence Network with Attetion Module\n",
    "## Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAv1aaG8Dnys"
   },
   "source": [
    "<h4><div style=\"text-align: right\"> Due date:  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ yscec by 9 PM in the form of [ID_Name_Lab12.ipynb]. </div></h4>\n",
    "\n",
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span> \n",
    "- Each team must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SULIl9bRDnys"
   },
   "source": [
    "<h2><span style=\"color:blue\">2014142243 차현수</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2JP0LD9nDnys",
    "outputId": "0c1a8aae-4843-435b-dbaa-9624d918ff03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2021-05-24 17:57:27.117013\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBvC_gonDnyu"
   },
   "source": [
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "*************************************************************\n",
    "::\n",
    "\n",
    "    [(>): input, (=): target, (<): output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "...\n",
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nLSIcUoZDnyu"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v0s8XW7Dnyv"
   },
   "source": [
    "### 1. Prepare data\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs. Download the data from <https://download.pytorch.org/tutorial/data.zip>. The file is a tab separated list of translation pairs:\n",
    "\n",
    "\n",
    "    I am cold.    J'ai froid.\n",
    "    \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1K3W2RxeTKih5IiT5PcIyWNZSwMqtYSGZ\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xn2soUCGDnyv",
    "outputId": "a76ed812-6017-4ceb-a52c-2fd70a6b1377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counted words: fra = 4345 eng = 2803\n",
      "['tu n es pas habillee .', 'you re not dressed .']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \")\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open('dataset/lab12/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\", input_lang.name, '=', input_lang.n_words, output_lang.name, '=', output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A8yVC59WDnyw"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4qXkB3DDnyw"
   },
   "source": [
    "### 2. Build the Seq2Seq model [5 points]\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kKXrIIxi0t-Nm5HfzOukqjzEp7yEXEpV\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "[sequence to sequence network](https://arxiv.org/abs/1409.3215) is a model in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a single vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDTwMr5TDnyy"
   },
   "source": [
    "#### Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.  \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PyKBEVl5jwQfB0I0P2kG8nTGQVZQdZEM\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "#### GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1467jVFRYbw1DYvVKeSyzGWLRmtlqpy8z\"  onerror=\"this.style.display='none'\" style=\"width: 700px;\"/><br><br>\n",
    "The GRU operates using a reset gate (r) and an update gate (z). The candidate state is created by using the previous hidden state and the current input. It is the reset gate that determines how the previous hidden state affects the candidate state. The newly created candidate state and the previous hidden state create a new hidden state, in which the update gate plays a role in balancing the two.\n",
    "\n",
    "#### LSTM vs GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1lzGTsIYvPWKNF-XaTevMaaZvjfgp9G35\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>\n",
    "\n",
    "| <center>LSTM</center> | <center>GRU</center>  |\n",
    "|:--------|--------|\n",
    "| LSTM has 3 gates (forget, input, output) | GRU has 2 gates (reset, update) |\n",
    "| There is an internal memory (cell state) | There is no cell state and only hidden state exists |\n",
    "| When making output, another non-linearity is applied | There is no additional non-linearity when making output  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PpL3bajyDnyy"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "\n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim)\n",
    "        #############\n",
    "\n",
    "    def forward(self, input, hn):\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        input = self.embedding(input).view(1, 1, -1)\n",
    "        output = input\n",
    "        output, hn = self.gru(output, hn)\n",
    "        #############\n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "        #############\n",
    "        return h0\n",
    "    \n",
    "hidden_dim = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGjPPHD7Dnyy"
   },
   "source": [
    "#### Decoder\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Rm_LlpEolCvPuzPWEFOZ-zdTfsgMbtu-\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence. Attention allows the decoder network to \"focus\" on a specific part of\n",
    "the encoder's outputs for every step and thus help the decoder choose the right output words. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18hsS8PAA7I3QaN9oOebfnMGAMhR-6EID\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1F1Y92uLvGaI6s-ygyNKNox4ZGiZmTZ3g\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "The attention weights are calculated using an another feed-forward layer which inputs the decoder's input and hidden state. And the calculated attention weight is multiplied to the corresponding hidden state of the encoder, respectively. Note that to actually create and train this layer we have to choose a maximum sentence length. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JEE23gtJf4XciJUXLt2R9lZtpRn8mYCN\"  onerror=\"this.style.display='none'\" /><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NfWr22rFDnyz"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        # attention\n",
    "        # Note that the column of the attention weights is MAX_LENGTH\n",
    "        # Note that concatenation is used when \"attn\" and \"attn_combine\" are created\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        self.max_length = MAX_LENGTH\n",
    "        self.attn = nn.Linear(self.hidden_dim * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        #############\n",
    "        \n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        self.gru = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        #############\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hn, encoder_outputs):\n",
    "        input = self.embedding(input).view(1, 1, -1)\n",
    "        input = self.dropout(input)\n",
    "        \n",
    "        # attention\n",
    "        # All specifications of the operations are described in the above figure (e.g. use ReLU)\n",
    "        # bmm is a operation which performs a batch matrix-matrix product\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((input[0], hn[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((input[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #############\n",
    "        \n",
    "        # gru\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        output = F.relu(output)\n",
    "        output, hn = self.gru(output, hn)\n",
    "        #############\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "        #############\n",
    "        return h0\n",
    "    \n",
    "decoder = AttnDecoderRNN(hidden_dim, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg2utJ-1Dnyz"
   },
   "source": [
    "### 3. loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KE-npWCODnyz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMr9RcDBDnyz"
   },
   "source": [
    "### 4. Write the evaluation code [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UO3UzfFCDnyz"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        #############\n",
    "        \n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device)\n",
    "        \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoded_words = []\n",
    "        \n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "            \n",
    "        decoder_hidden = encoder_hidden\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        #############\n",
    "\n",
    "        return decoded_words\n",
    "    \n",
    "def evaluateRandomly():\n",
    "    pair = random.choice(pairs)\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7wNENhXDnyz"
   },
   "source": [
    "### 5 . Write the code to train the model [3 points]\n",
    "\n",
    "During training, use the ``Teacher forcing`` concept in addition to a naive approach. In other words, instead of using the decoder's guess as the next input, the real target outputs are also used sometimes. This shows faster convergene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9x83bK3GDnyz",
    "outputId": "34533830-f22b-4080-bc51-582d4993b8e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* iter1000 *************************\n",
      "loss 16.1061\n",
      "> tu es responsable de ce resultat .\n",
      "= you are responsible for the result .\n",
      "< you re not a . <EOS>\n",
      "\n",
      "************************* iter2000 *************************\n",
      "loss 14.1632\n",
      "> vous etes tres habiles .\n",
      "= you re very clever .\n",
      "< you re very very . <EOS>\n",
      "\n",
      "************************* iter3000 *************************\n",
      "loss 29.4210\n",
      "> je ne suis pas encore pret .\n",
      "= i m not ready yet .\n",
      "< i m not sure . <EOS>\n",
      "\n",
      "************************* iter4000 *************************\n",
      "loss 24.7786\n",
      "> vous etes le champion n est ce pas ?\n",
      "= you re the champion aren t you ?\n",
      "< you re the oldest aren t you ? <EOS>\n",
      "\n",
      "************************* iter5000 *************************\n",
      "loss 25.3934\n",
      "> nous sommes bourrees .\n",
      "= we re smashed .\n",
      "< we re in the . <EOS>\n",
      "\n",
      "************************* iter6000 *************************\n",
      "loss 13.4960\n",
      "> c est mon ami intime .\n",
      "= he is my close friend .\n",
      "< he s my friend . <EOS>\n",
      "\n",
      "************************* iter7000 *************************\n",
      "loss 5.8938\n",
      "> elle est mere isolee de deux enfants .\n",
      "= she s a single mother of two .\n",
      "< she is in the boss of my father . <EOS>\n",
      "\n",
      "************************* iter8000 *************************\n",
      "loss 14.8707\n",
      "> je me rejouis de vous avoir finalement rencontres .\n",
      "= i m glad to finally meet you .\n",
      "< i m sorry i m sorry . <EOS>\n",
      "\n",
      "************************* iter9000 *************************\n",
      "loss 29.7055\n",
      "> il commence a se sentir desespere .\n",
      "= he s starting to feel desperate .\n",
      "< he s going to need some your friend . <EOS>\n",
      "\n",
      "************************* iter10000 *************************\n",
      "loss 9.7678\n",
      "> vous etes grandes .\n",
      "= you are big .\n",
      "< you re a liar . <EOS>\n",
      "\n",
      "************************* iter11000 *************************\n",
      "loss 22.7319\n",
      "> il est beau et intelligent .\n",
      "= he is handsome and clever .\n",
      "< he s a very tall . <EOS>\n",
      "\n",
      "************************* iter12000 *************************\n",
      "loss 2.0324\n",
      "> je suis impatient de vous revoir .\n",
      "= i am looking forward to seeing you again .\n",
      "< i am surprised to see you again . <EOS>\n",
      "\n",
      "************************* iter13000 *************************\n",
      "loss 25.0367\n",
      "> il tremble de froid .\n",
      "= he s shivering because of the cold .\n",
      "< he is well . <EOS>\n",
      "\n",
      "************************* iter14000 *************************\n",
      "loss 5.4969\n",
      "> je prends du poids .\n",
      "= i m gaining weight .\n",
      "< i m going to be a french . <EOS>\n",
      "\n",
      "************************* iter15000 *************************\n",
      "loss 6.4772\n",
      "> c est un enseignant experimente .\n",
      "= he is an experienced teacher .\n",
      "< he s a born artist . <EOS>\n",
      "\n",
      "************************* iter16000 *************************\n",
      "loss 8.6489\n",
      "> vous etes bon menteur .\n",
      "= you re a good liar .\n",
      "< you re a good liar . <EOS>\n",
      "\n",
      "************************* iter17000 *************************\n",
      "loss 1.2384\n",
      "> je suis encore endormi .\n",
      "= i m still sleepy .\n",
      "< i m still . <EOS>\n",
      "\n",
      "************************* iter18000 *************************\n",
      "loss 9.4224\n",
      "> tu es trop jeune pour te marier .\n",
      "= you re too young to marry .\n",
      "< you re too old to you . <EOS>\n",
      "\n",
      "************************* iter19000 *************************\n",
      "loss 10.3416\n",
      "> je suis tres content de ma nouvelle voiture .\n",
      "= i m very happy with my new car .\n",
      "< i am very proud of my old age . <EOS>\n",
      "\n",
      "************************* iter20000 *************************\n",
      "loss 10.7746\n",
      "> je ne suis pas convaincu .\n",
      "= i m not persuaded .\n",
      "< i m not perfect . <EOS>\n",
      "\n",
      "************************* iter21000 *************************\n",
      "loss 31.5056\n",
      "> il est expert en litterature francaise .\n",
      "= he is well acquainted with french literature .\n",
      "< he s a terrible now . <EOS>\n",
      "\n",
      "************************* iter22000 *************************\n",
      "loss 6.0464\n",
      "> vous etes mon heroine .\n",
      "= you are my hero .\n",
      "< you are my friend . <EOS>\n",
      "\n",
      "************************* iter23000 *************************\n",
      "loss 3.6363\n",
      "> il est en train de lire un roman .\n",
      "= he s reading a novel now .\n",
      "< he is a novel now . <EOS>\n",
      "\n",
      "************************* iter24000 *************************\n",
      "loss 7.2254\n",
      "> c est un musicien de grande qualite .\n",
      "= he s a very fine musician .\n",
      "< he s a funny guy . <EOS>\n",
      "\n",
      "************************* iter25000 *************************\n",
      "loss 17.3497\n",
      "> il est tres impatient de s y rendre .\n",
      "= he is very eager to go there .\n",
      "< he is very eager to go there . <EOS>\n",
      "\n",
      "************************* iter26000 *************************\n",
      "loss 3.9307\n",
      "> elle est d un naturel calme .\n",
      "= she s a quiet person .\n",
      "< she is a little a man of being a hat\n",
      "\n",
      "************************* iter27000 *************************\n",
      "loss 5.4371\n",
      "> il a des problemes .\n",
      "= he is in trouble .\n",
      "< he s in trouble . <EOS>\n",
      "\n",
      "************************* iter28000 *************************\n",
      "loss 0.4579\n",
      "> vous etes fort serviable .\n",
      "= you re very helpful .\n",
      "< you re very helpful . <EOS>\n",
      "\n",
      "************************* iter29000 *************************\n",
      "loss 24.6469\n",
      "> nous sommes tellement contents de t avoir ici !\n",
      "= we re so glad to have you here .\n",
      "< we re so glad to have you . <EOS>\n",
      "\n",
      "************************* iter30000 *************************\n",
      "loss 12.7470\n",
      "> je suis impatient de la prochaine fois .\n",
      "= i m looking forward to the next time .\n",
      "< i m looking forward to the new york . <EOS>\n",
      "\n",
      "************************* iter31000 *************************\n",
      "loss 6.0454\n",
      "> ce n est pas un homme ordinaire .\n",
      "= he is no ordinary man .\n",
      "< he is no ordinary man . <EOS>\n",
      "\n",
      "************************* iter32000 *************************\n",
      "loss 6.7724\n",
      "> vous etes inquietes n est ce pas ?\n",
      "= you re worried aren t you ?\n",
      "< you re disappointed aren t you ? <EOS>\n",
      "\n",
      "************************* iter33000 *************************\n",
      "loss 19.0108\n",
      "> j ai honte .\n",
      "= i am ashamed .\n",
      "< i am ashamed . <EOS>\n",
      "\n",
      "************************* iter34000 *************************\n",
      "loss 1.2843\n",
      "> nous sommes enfin seules .\n",
      "= we re finally alone .\n",
      "< we re alone alone . <EOS>\n",
      "\n",
      "************************* iter35000 *************************\n",
      "loss 4.3829\n",
      "> elle a peur de retomber malade .\n",
      "= she is afraid of falling ill again .\n",
      "< she is afraid to do anything . <EOS>\n",
      "\n",
      "************************* iter36000 *************************\n",
      "loss 3.1616\n",
      "> je ne suis pas ton professeur .\n",
      "= i m not your teacher .\n",
      "< i m not your teacher . <EOS>\n",
      "\n",
      "************************* iter37000 *************************\n",
      "loss 6.4047\n",
      "> vous etes trop poli .\n",
      "= you re too polite .\n",
      "< you re too loud . <EOS>\n",
      "\n",
      "************************* iter38000 *************************\n",
      "loss 4.1265\n",
      "> je ne suis pas riche .\n",
      "= i m not wealthy .\n",
      "< i m not rich . <EOS>\n",
      "\n",
      "************************* iter39000 *************************\n",
      "loss 7.7163\n",
      "> ils sont fatigues .\n",
      "= they re tired .\n",
      "< they are tired . <EOS>\n",
      "\n",
      "************************* iter40000 *************************\n",
      "loss 3.3861\n",
      "> vous etes inquiets n est ce pas ?\n",
      "= you re worried aren t you ?\n",
      "< you re worried aren t you ? <EOS>\n",
      "\n",
      "************************* iter41000 *************************\n",
      "loss 22.1385\n",
      "> je suis a toi .\n",
      "= i m yours .\n",
      "< i m ready for you . <EOS>\n",
      "\n",
      "************************* iter42000 *************************\n",
      "loss 2.0673\n",
      "> j ai trente ans de plus que toi .\n",
      "= i m thirty years older than you .\n",
      "< i m thirty years older than you . <EOS>\n",
      "\n",
      "************************* iter43000 *************************\n",
      "loss 7.0428\n",
      "> il ne se presente pas aux prochaines elections .\n",
      "= he is not running in the coming election .\n",
      "< he is not going to the police . <EOS>\n",
      "\n",
      "************************* iter44000 *************************\n",
      "loss 3.0243\n",
      "> je suis votre voisin .\n",
      "= i m your neighbor .\n",
      "< i m your boss . <EOS>\n",
      "\n",
      "************************* iter45000 *************************\n",
      "loss 3.1917\n",
      "> vous etes trop maigrichonne .\n",
      "= you re too skinny .\n",
      "< you re too skinny . <EOS>\n",
      "\n",
      "************************* iter46000 *************************\n",
      "loss 5.7533\n",
      "> vous etes tres effrontees .\n",
      "= you re very forward .\n",
      "< you re very nice . <EOS>\n",
      "\n",
      "************************* iter47000 *************************\n",
      "loss 0.1937\n",
      "> vous etes toujours vivantes .\n",
      "= you re still alive .\n",
      "< you re still alive . <EOS>\n",
      "\n",
      "************************* iter48000 *************************\n",
      "loss 2.0108\n",
      "> vous etes presque aussi grand que tom .\n",
      "= you re almost as tall as tom .\n",
      "< you re almost as tall as tom . <EOS>\n",
      "\n",
      "************************* iter49000 *************************\n",
      "loss 1.1552\n",
      "> vous etes l ainee .\n",
      "= you re the oldest .\n",
      "< you re the oldest . <EOS>\n",
      "\n",
      "************************* iter50000 *************************\n",
      "loss 0.2865\n",
      "> il quitte narita pour hawaii ce soir .\n",
      "= he is leaving narita for hawaii this evening .\n",
      "< he is leaving for this evening . <EOS>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f898035f280>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 50000\n",
    "print_every = 1000\n",
    "plot_every =100\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    #############\n",
    "    # CODE HERE #\n",
    "    #############\n",
    "    # Load data\n",
    "    training_pair = training_pairs[iter-1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = 0\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    print_loss_total += loss.item() / target_length\n",
    "    plot_loss_total += loss.item() / target_length\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('*'*25, 'iter%d'%iter, '*'*25)\n",
    "        print('loss %.4f'%loss)\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        evaluateRandomly()\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "#################################################\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrccVZ1gDny0"
   },
   "source": [
    "### *References*\n",
    "[1] [practical pytorch](https://github.com/spro/practical-pytorch)(https://github.com/spro/practical-pytorch)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EEE4423_lab11_Seq2Seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
