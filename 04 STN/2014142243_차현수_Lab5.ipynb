{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXFfI0wi2OWc"
   },
   "source": [
    "> ### EEE4423: Signal Processing Lab\n",
    "\n",
    "# LAB \\#5: Limitation: Spatial Transformer Network(STN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_TAt1ul2OWd"
   },
   "source": [
    "<h4><div style=\"text-align: right\"> Due date:  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ yscec by 9 PM in the form of [ID_Name_Lab5.ipynb]. </div></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAd1A1R52OWe"
   },
   "source": [
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span> \n",
    "- Each team must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v-RdgrF2OWf"
   },
   "source": [
    "<h2><span style=\"color:blue\">2014142243 차현수</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsMlF0RR2OWg",
    "outputId": "6ba88b6c-4541-49f8-e21f-086706c61e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2021-06-19 17:25:55.595424\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxWW19iY2OWm"
   },
   "source": [
    "## Spatial Transformer Network for classification of distorted MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzBuF0OV2OWn"
   },
   "source": [
    "## Spatial Transformer Network(STN) [1]\n",
    ">- CNNs are limited by the lack of ability to be spatially invariant to the input data\n",
    ">- Learnable module which explicitly allows the spatial manipulation of data within the network\n",
    ">- This differentiable module can be inserted into existing convolutional architectures\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1GV2Ix6wuikWdq6-tGkZv2vMQbqMokDbf\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "### STN module\n",
    ">1. Localization Network\n",
    ">>- With given input feature map, this network outputs the parameters of the spatial transformation (e.g. 6 parameters for affine transformation)\n",
    ">>- Reference for affine transformation : [2],[3] <br>\n",
    ">> <img src=\"https://docs.google.com/uc?export=download&id=1qho08Gzea5qDTpmsnii0rvwiLzwy54K6\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    ">2. Parameterised sampling grid (Grid generator)\n",
    ">>- Set of points where the input feature map is sampled to produce the transformation which is a output of localization network  \n",
    ">>- Target coordinate and source coordinate are normalised ($ -1\\le(x_i^t, y_i^t)\\le1$,$ -1\\le(x_i^s, y_i^s)\\le1$ )\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1pRWzwevo1KjWi3WIC4K8SCkK4oYCD7FZ\" alt=\"no_image\" style=\"width: 500px;\"/>\n",
    ">3. Differentiable Image Sampling (Sampler)\n",
    ">>- Ouput feature map is produced by differentiable bilinear interpolation with input feature map and parameterised sampling grid\n",
    "    \n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1EjoZ6CVLTD3QNl1CKbg1w3YiNf1CzOmH\" alt=\"no_image\" style=\"width: 900px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AoHVrM52OWo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as v_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import requests\n",
    "import os \n",
    "import copy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g84YJUCD2OWr",
    "outputId": "006c73a3-31d5-4b0c-982d-34a203b6f27a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gaJ_3Gv2OWv"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "num_epoch = 60\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHuhiUfN2OWy"
   },
   "source": [
    "### 1. Dataset (Distorted MNIST, details in Appendix A.4 Distorted MNIST) [1point]\n",
    ">- Generate RTS(rotated, translated, scaled) MNIST \n",
    ">>- Use *torchvision.transforms*\n",
    ">>- Randomly rotating between $-45^\\circ, 45^\\circ$\n",
    ">>- Randomly scaling the digit by a factor of between $0.7,1.2$\n",
    ">>- Placing the digit in a random location in a $40\\times40$ region of image's center\n",
    ">>- Zerp padding to increase image's size for the digit's translation ($80\\times80$ image)\n",
    ">>- Images to tensor \n",
    ">>- Normalize data with MNIST dataset's mean and standard deviation printed in the 5th cell below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFHJ42zM2OW0"
   },
   "source": [
    "#### 1.1 Write codes for dataset's transformation [1 point]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJaXUT-12OW1",
    "outputId": "830a480e-708b-4bf5-b508-cfb78023e757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: DEFINE DATASET\n",
      "MNIST mean:  tensor(0.1307)\n",
      "MNIST std:  tensor(0.3081)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py:64: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "print('STEP 1: DEFINE DATASET')\n",
    "\n",
    "import random\n",
    "train_dataset = dset.MNIST(\"./data/mnist\", train=True,\n",
    "                        #############\n",
    "                        # CODE HERE #\n",
    "                        transform=transforms.Compose([\n",
    "                            # Randomly Rotating between -45 to 45 degree\n",
    "                            # Randomly scaling between 0.7 to 1.2\n",
    "                            transforms.RandomAffine(degrees=(-45, 45), scale=(0.7, 1.2)),\n",
    "                            # Randomly located in a 40 x 40 region of image's centre\n",
    "                            transforms.RandomCrop(80, padding=40),\n",
    "                            # Zero Padding to increase image's size to 80 x 80\n",
    "                            transforms.CenterCrop(80),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "                        ]),\n",
    "                        #############\n",
    "                        target_transform=None,\n",
    "                        download=True)\n",
    "test_dataset = dset.MNIST(\"./data/mnist\", train=False,\n",
    "                        #############\n",
    "                        # CODE HERE #\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.RandomAffine(degrees=(-45, 45), scale=(0.7, 1.2)),\n",
    "                            transforms.RandomCrop(80, padding=40),\n",
    "                            transforms.CenterCrop(80),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "                        ]),\n",
    "                        #############\n",
    "                        target_transform=None,\n",
    "                        download=False)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "#dataset mean and std for normalization\n",
    "print('MNIST mean: ',train_dataset.train_data.float().mean()/255)\n",
    "print('MNIST std: ',train_dataset.train_data.float().std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmdruxLV2OW5",
    "outputId": "55ecb3fe-8846-4515-8299-44c879306370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size: {}'.format(len(train_dataset)))\n",
    "print('Test dataset size: {}'.format(len(test_dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9PliNxe2OW9"
   },
   "source": [
    "###  Visualize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7MVg2wH2OW-",
    "outputId": "e0a1b9cb-c9db-477d-a825-91f0cacd88e7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "\n",
    "for i in range(train_size):\n",
    "    sample = train_dataset[i]\n",
    "    figure.add_subplot(1,4,i+1).set_title('Label:{}'.format(sample[1]))\n",
    "    imgplot = plt.imshow((sample[0].squeeze(0).cpu()+1)/2)\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9wwuE7t2OXC",
    "outputId": "835d6b06-3445-4404-f5de-6f5869a060f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: LOADING DATASET\n"
     ]
    }
   ],
   "source": [
    "print('STEP 2: LOADING DATASET')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True,num_workers=4,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size, shuffle=False,num_workers=4,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkYr7Cp_2OXG"
   },
   "source": [
    "##  Model\n",
    "### 2. CNN Model Setup (details in Appendix A.4 Distorted MNIST) [3points]\n",
    ">\n",
    ">1. CNN \n",
    ">>- 2 convolutional layers and 2 max-pooling layers before final classification layer\n",
    ">>- Two conv layers have 32 and 64 filters and use ReLU \n",
    ">2. Classifier\n",
    ">>- 2 fully-connected layers and the number of input features to the last layer is 128\n",
    ">>- Also use ReLU as an activation function\n",
    ">3. ST module\n",
    ">>- At the beginning of the network\n",
    ">>- 2 convolutional layer and 2 fully-connected layer in localization network\n",
    ">>- Initialize the *fc_loc*'s final regression layer with identity transformation\n",
    ">>- Produce affine transformation parameters for RTS dataset\n",
    ">>- Reference for grid generator function: [4] <br>\n",
    ">>- Reference for sampler function: [5] <br>\n",
    ">\n",
    "> **++Hint: All learnable parameters' sizes of model are in the 10th cell below** <br>\n",
    "> **++For RTS datasets, the network has average pooling layer after the ST module to downsample the output of the transformer by a factor of 2**\n",
    "\n",
    "| **Layer** | **Kernel size** | **stride** | **padding** |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| 1st Conv of *cnn* | 9 | 1 | 0 |\n",
    "| 2nd Conv of *cnn* | 7 | 1 | 0 |\n",
    "| 1st Conv of *localization* | 5 | 1 | 0 |\n",
    "| 2nd Conv of *localization* | 5 | 1 | 0 |\n",
    "| AvgPool | 2 | 2 | 0 |\n",
    "| MaxPool | 2 | 2 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYHuW3KR2OXH"
   },
   "source": [
    "#### 2.1 Write codes for the model class (STN_CNN) [3 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pT5hnJ6l2OXI",
    "outputId": "8ea84272-477f-4eb9-8574-c38f600d70c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: CREATE MODEL CLASS (STN_CNN)\n"
     ]
    }
   ],
   "source": [
    "print('STEP 3: CREATE MODEL CLASS (STN_CNN)')\n",
    "        \n",
    "class STN_CNN(nn.Module):\n",
    "    #############\n",
    "    # CODE HERE #\n",
    "    def __init__(self):\n",
    "        super(STN_CNN, self).__init__()\n",
    "        # CNN Module\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=9, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1600, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "        # Localization-network (Spatial transformer)\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(20, 20, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Regressor (for Affine Matrix)\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(in_features=17 * 17 * 20 * 4, out_features=20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, 3 * 2)\n",
    "        )\n",
    "        # Initialize\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        # identity transformation.\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "        \n",
    "        self.avg = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    # ST Network Module\n",
    "    def stmodule(self, x, im_size):\n",
    "        x_loc = self.localization(x)\n",
    "        # torch.Size([256, 20, 17, 17])\n",
    "        x_viewloc = x_loc.view(-1, 17*17*20*4)\n",
    "        theta = self.fc_loc(x_viewloc)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        x = x.view(im_size, -1, 80, 80)\n",
    "        grid_out = F.affine_grid(theta, x.size())\n",
    "        out = F.grid_sample(x, grid_out)\n",
    "        return out, theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st, Spatial Transformer Model\n",
    "        # torch.Size([256, 1, 80, 80])\n",
    "        out, theta = self.stmodule(x, im_size=64)\n",
    "        # At the end of the ST Module, enter the average pooling layer\n",
    "        out = self.avg(out)\n",
    "        # Change Dimension\n",
    "        print(out.size())#torch.Size([64, 4, 40, 40])\n",
    "        out = out.view(256, 1, 40, 40)\n",
    "        # Regular CNN Model\n",
    "        out = self.cnn(out)\n",
    "        out = out.view(-1, 1600)\n",
    "        out = self.classifier(out)\n",
    "        # Reduce softmax loss.\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out, theta\n",
    "    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjmaoyEv2OXM",
    "outputId": "bf5a432d-493d-4d2d-d8e6-fc3c8783cde6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: INSTANTIATE MODEL CLASS\n",
      "\n",
      "STN_CNN's state_dict:\n",
      "cnn.0.weight \t torch.Size([32, 1, 9, 9])\n",
      "cnn.0.bias \t torch.Size([32])\n",
      "cnn.3.weight \t torch.Size([64, 32, 7, 7])\n",
      "cnn.3.bias \t torch.Size([64])\n",
      "classifier.0.weight \t torch.Size([128, 1600])\n",
      "classifier.0.bias \t torch.Size([128])\n",
      "classifier.2.weight \t torch.Size([10, 128])\n",
      "classifier.2.bias \t torch.Size([10])\n",
      "localization.0.weight \t torch.Size([20, 1, 5, 5])\n",
      "localization.0.bias \t torch.Size([20])\n",
      "localization.4.weight \t torch.Size([20, 20, 5, 5])\n",
      "localization.4.bias \t torch.Size([20])\n",
      "fc_loc.0.weight \t torch.Size([20, 23120])\n",
      "fc_loc.0.bias \t torch.Size([20])\n",
      "fc_loc.2.weight \t torch.Size([6, 20])\n",
      "fc_loc.2.bias \t torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print('STEP 4: INSTANTIATE MODEL CLASS\\n')\n",
    "model = STN_CNN()\n",
    "\n",
    "print(\"STN_CNN's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvfNJkk02OXQ"
   },
   "source": [
    "###  Parameter updates\n",
    "#### Scheduling (Annealing) the learning rate [6]\n",
    ">- In training deep networks, it is usually helpful to anneal the learning rate over time\n",
    ">>- With high learning rate, the optimizing system can't settle down into deeper parts of the loss function\n",
    ">- When to decay can be tricky\n",
    ">>- Slowly : Wasting computation with little improvement for a long time\n",
    ">>- Aggressively: Cooling too quickly, unable to find the best point\n",
    ">- 3 common types\n",
    ">>1. Step decay: Reduce the learning rate by some factor every few epochs (e.g. half every 5 epochs, or by 0.1 every 10 epochs)\n",
    ">>2. Exponential decay: In the form of mathematical formulation $\\alpha = \\alpha_0\\exp^{-kt}$, where $\\alpha_0, k$ are hyperparameters and $t$ is the iteration number(or units of epochs)\n",
    ">>3. $1/t$ decay : In the form of mathematical formulation $\\alpha = \\alpha_0/(1+kt)$, where $\\alpha_0, k$ are hyperparameters and $t$ is the iteration number\n",
    ">- In practice, the step decay is slightly preferable\n",
    "\n",
    "#### How to adjust learning rate in pytorch [7]\n",
    ">- *torch.optim.lr_scheduler* provides several methods based on the number of epochs\n",
    ">- For example, the step decay can be implemented by *torch.optim.lr_scheduler.StepLR* class (See reference for more types)\n",
    ">- We use *ReduceLROnPlateau* class in this lab which allows dynamic learning rate adjusting based on our validation measurements\n",
    ">>- Reduce the learning rate when our metric has stopped improving \n",
    ">>- The learning rate is reduced if no improvement of our metric is seen for a 'patience' number of epochs\n",
    ">>- See reference for more details \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLusumF82OXR",
    "outputId": "5830bff3-6ed8-41a4-f499-72bab0e13ad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: INSTANTIATE OPTIMIZER CLASS\n"
     ]
    }
   ],
   "source": [
    "print('STEP 5: INSTANTIATE OPTIMIZER CLASS')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = 0.1, patience=6)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvw4SWmz2OXY"
   },
   "source": [
    "### 3.Train/Test [2points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCEa1yPU2OXa"
   },
   "source": [
    "#### 3.1Tirain the STN_CNN model and print accuracy for every epochs [2 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyKtHA002OXb",
    "outputId": "0e82495e-393f-4624-e3c6-afd50cd69968",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: INSTANTIATE LOSS CLASS\n",
      "STEP 7: TRAIN THE MODEL\n",
      "60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:3890: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:3828: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "Train Epoch: 0 [0/60000 (0%) / Learning rate:0.001]\tLoss:2.301237  \n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "Train Epoch: 0 [51200/60000 (85%) / Learning rate:0.001]\tLoss:0.816251  \n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n",
      "torch.Size([64, 4, 40, 40])\n",
      "torch.Size([256, 1, 40, 40])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cc92b1fc2cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('STEP 6: INSTANTIATE LOSS CLASS')\n",
    "#############\n",
    "# CODE HERE #\n",
    "# I will use NLLLoss, the code embeded already in STEP 7 and STEP 3 at the end of the Class.\n",
    "#############\n",
    "\n",
    "# Model to GPU\n",
    "model.to(device0)\n",
    "\n",
    "\n",
    "print('STEP 7: TRAIN THE MODEL')\n",
    "import os                                      \n",
    "file = './best_model_STN.tar'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    os.remove(file)\n",
    "\n",
    "print(train_size)\n",
    "best_acc = 0\n",
    "theta_list = []\n",
    "for epoch in range(num_epoch):\n",
    "    #TRAIN\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for j,(img,label) in enumerate(train_loader):\n",
    "        \n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        img, label = img.to(device0), label.to(device0)\n",
    "        optimizer.zero_grad()\n",
    "        output, theta = model(img)\n",
    "        theta_list.append(theta)\n",
    "        loss = F.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #############\n",
    "        \n",
    "        \n",
    "        if j % 200 == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%) / Learning rate:{}]\\tLoss:{:.6f}  '.format(\n",
    "                    epoch, j * len(img), train_size,\n",
    "                    100. * j / len(train_loader),get_lr(optimizer), loss.item()))\n",
    "           \n",
    "                    \n",
    "    #Test\n",
    "    model.eval()\n",
    "    #############\n",
    "    # CODE HERE #\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(device0), label.to(device0)\n",
    "        output, _ = model(img)\n",
    "\n",
    "        # Sum of all of loss values\n",
    "        loss += F.nll_loss(output, label, size_average=False).item()\n",
    "        prediction = output.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset), accuracy))\n",
    "    \n",
    "    #############\n",
    "    print('////Epoch elapsed time: {}////\\n'.format(time.time() - start))  \n",
    "    \n",
    "    if accuracy > best_acc :\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "            \n",
    "            }, './best_model_STN.tar')\n",
    "       \n",
    "        best_acc = accuracy\n",
    "    \n",
    "    scheduler.step(accuracy)\n",
    "#print(theta[0])\n",
    "#print(theta[-1])\n",
    "print(\"Finish Calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhPP8ATu2OXg"
   },
   "source": [
    "### 4. Visualize original inputs and transformed inputs with best pre-trained model  [3points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZj6oLi62OXi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = STN_CNN()\n",
    "checkpoint = torch.load('./best_model_STN.tar') # torch.load('./pretrain/lab5/best_model_STN.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6WD4pCv2OXk"
   },
   "source": [
    "#### Our pretrained model's best accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UB4GmH2g2OXl",
    "outputId": "83e334ed-b093-4d82-d0be-c29ad5c4ee29"
   },
   "outputs": [],
   "source": [
    "print('Best accuracy of our model with ST module: ', checkpoint['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F5ppqXP2OXr"
   },
   "outputs": [],
   "source": [
    "# Tensor image to array image\n",
    "\n",
    "def reprocess_image(img):\n",
    "    \n",
    "    img_re = copy.copy(img.cpu().data.numpy())\n",
    "    \n",
    "    mean = [-0.1307,-0.1307,-0.1307]\n",
    "    std = [1/0.3081,1/0.3081,1/0.3081]\n",
    "    \n",
    "    for c in range(3):\n",
    "        img_re[c,:,:] /= std[c]\n",
    "        img_re[c,:,:] -= mean[c]\n",
    "        \n",
    "    img_re[img_re > 1] = 1\n",
    "    img_re[img_re < 0] = 0\n",
    "    \n",
    "    img_re = img_re.transpose(1,2,0)\n",
    "    \n",
    "    return img_re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suci-kWM2OXt"
   },
   "source": [
    "#### 4.1 Write codes for visualization of original inputsa and transformed inputs [3 points]\n",
    ">- VisualizeSTN class with an input of our pretrained model\n",
    ">- *forward_stn*: Forward pass of our pretrained STN module to produce transformed inputs\n",
    ">- *visualize*: Visualizing the original inputs and the transformed ones in a grid \n",
    ">>1. Forward pass of STN module to produce the transformed inputs\n",
    ">>2. Unnormalize both images using *reprocess_image* function\n",
    ">>3. Make grids of them \n",
    ">>4. Visualize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcdkczCj2OXu"
   },
   "outputs": [],
   "source": [
    "class VisualizeSTN():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.to(device0)\n",
    "        self.model.eval()\n",
    "    \n",
    "    # Output transformed inputs\n",
    "    def forward_stn(self, img):\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        # Perform the usual forward pass\n",
    "        # Get a theta of training data\n",
    "        transform_input, theta = self.model.stmodule(img, im_size=4)\n",
    "        #print(theta)\n",
    "        transform_input = transform_input.view(16, -1, 80, 80)\n",
    "        return transform_input\n",
    "        #############\n",
    "    \n",
    "    def visualize(self, img):\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        trans_t = self.forward_stn(img)\n",
    "        # by using reprocess function, make unnormalization\n",
    "        grid_in = reprocess_image(v_utils.make_grid(img, nrow=4))\n",
    "        grid_out = reprocess_image(v_utils.make_grid(trans_t, nrow=4))\n",
    "        \n",
    "        f, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(grid_in)\n",
    "        ax[0].set_title('Original images')\n",
    "        ax[1].imshow(grid_out)\n",
    "        ax[1].set_title('Transformed images')\n",
    "        #############\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mkGExZk2OXx",
    "outputId": "b34b0bc7-6b72-407d-a7d1-868e3cbc1aaf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_stn = VisualizeSTN(model)\n",
    "for i, (image,label) in enumerate(test_loader):\n",
    "    \n",
    "    img = image[:16].to(device0)\n",
    "    visualize_stn.visualize(img)\n",
    "    \n",
    "    if (i+1) == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEjhSogi2OX0"
   },
   "source": [
    "### 5.Comparison with the cnn model without ST module [1point]\n",
    ">- Model composed of cnn and classifier modules same with our pretrained CNN_STN model \n",
    ">- Use *cnn* and *classifier* modules you implemented \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pecTLkVL2OX0"
   },
   "source": [
    "#### 5.1 Write codes for the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnhjymoE2OX2"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# CODE HERE #\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # CNN Module\n",
    "        # Same as upper model\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=9, stride=1),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1600, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(-1, 1600)\n",
    "        out = self.classifier(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        out = out.view(256, -1)\n",
    "        return out\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0dtTNMV2OX4",
    "outputId": "a6c0f479-8a8c-425b-ae7d-ca71e688e5be"
   },
   "outputs": [],
   "source": [
    "model_nostn = CNN()\n",
    "\n",
    "print(\"CNN's state_dict:\")\n",
    "for param_tensor in model_nostn.state_dict():\n",
    "    print(param_tensor, \"\\t\", model_nostn.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTHfuIMJ2OX7"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_nostn.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = 0.1, patience=6)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-jXzO1Y2OX-"
   },
   "source": [
    "#### 5.2 Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1oDqtGl2OX_",
    "outputId": "f7052aca-d4bc-425c-80d4-0ec8142a84e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# CODE HERE #\n",
    "model_nostn.to(device0)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    #TRAIN\n",
    "    model_nostn.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for j,(img,label) in enumerate(train_loader):\n",
    "        img, label = img.to(device0), label.to(device0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_nostn(img)\n",
    "        loss = F.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%) / Learning rate:{}]\\tLoss:{:.6f}  '.format(\n",
    "                    epoch, j * len(img), train_size,\n",
    "                    100. * j / len(train_loader),get_lr(optimizer), loss.item()))\n",
    "                    \n",
    "    #Test\n",
    "    model_nostn.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(device0), label.to(device0)\n",
    "        output = model_nostn(img)\n",
    "        \n",
    "        loss += F.nll_loss(output, label, size_average=False).item()\n",
    "        prediction = output.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset),accuracy))\n",
    "    print('////Epoch elapsed time: {}////\\n'.format(time.time() - start))  \n",
    "    \n",
    "    if accuracy > best_acc :\n",
    "        best_acc = accuracy\n",
    "    \n",
    "    scheduler.step(accuracy)\n",
    "print(\"Finish Calculation\")\n",
    "\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgPo5F4x2OYC"
   },
   "source": [
    "#### Best accuracy of the model_nostn without ST module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAd8_Ss42OYC",
    "outputId": "bcd5d035-87c3-4d1e-b580-8c565d463798"
   },
   "outputs": [],
   "source": [
    "print('Best accuracy of our model_nostn without ST module: ', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Result report by written Cha, Hyunsoo\n",
    "\n",
    "Analyze Algorithm\n",
    "======\n",
    "This experiment is a direct implementation of the Spatial Transformer Network. CNN alone has certain limitations in finding distorted handwriting data. To overcome these limitations, a spatial transducer has been fabricated in DeepMind. Space converters are simpler than I think. \n",
    "\n",
    "First, the distorted MNIST dataset passes through the localization network. I use affine transformations to calculate how distorted this image is and which values can be corrected by matrix computation. A total of six parameters exist, and the unchanging matrix is equal to the same matrix. A total of nine transformations exist, and six parameter values learned through the Localization Network allow mine to estimate how distorted the data is. The functions provided by Pytorch can then be used to implement Grid Generator and Sampler. The Localization network, Grid generator, and sampler are usually called the Spatial Transformer Network Module. Through this module, distorted MNIST dataset serves as a spatial transformation to make it easier for CNN Networks to recognize. In the rest of the network, CNN and classifier networks are implemented.\n",
    "\n",
    "Entering the network in detail is as follows. The Localization Network stacks two convolutional layers, and also uses the max pooling layer and dropout, and the relu function. The reason for adding dropout layers was to prevent excessive tuning. The image passed through the Localization Network passes through fc_loc. A network for regressor of affine matrix. It consists of two full-connected layers. This results in estimating theta value and based on it, I modify the distorted MNIST Dataset to allow CNN to estimate with higher accuracy. Two convolutional layers, two maxpooling layers, and a relu via CNN network then the network proceeds to learn.\n",
    "\n",
    "The values can be estimated after passing through the Full-Connected layer via the Classifier. I implemented it in a way that minimizes loss through log softmax. Optimization was carried out using Adam optimizer. \n",
    "\n",
    "Report & Discussion\n",
    "======\n",
    "Accuracy results using the STN module were approximately 98%. The accuracy of CNN networks without STN modules differed significantly in the 71% range. Visualization was conducted to see if the space converter worked properly, but the conversion was not as much as expected. Changes in size and angle were observed somewhat, but they were not dramatic. This reason appears to be a poor estimation of Theta values. As a result of tracking directly through print statements to examine changes in Theta values, Theta matrices showed some tendency similar to Identity matrices. In other words, it is natural that the human eye does not feel much of a difference in view even if space is transformed through the Theta matrix. Then I also need to think about why Theta values did not show much difference. In one's opinion, Batch size is considerably large at 256, which leads to some errors in the transformation of dimensions between networks. To equate each layer and function and dimension, I use the view function more than other models, which I estimate resulted in poor accuracy in estimating. Batch size was given in the task and thus could not be modified, and based on a deeper understanding, it should be able to derive high accuracy while using fewer view functions.\n",
    "\n",
    "For reference, I observed an improvement of 2% in accuracy when the Dropout function was applied to CNN without the Dropout function compared to accuracy. In addition, for the second model, Only CNN model, I were able to achieve significantly higher accuracy compared to the past with 10% to 70% accuracy. In other words, the use of the dropout function to avoid excessive tuning in the network has resulted in the dropout rather disrupting the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLpVgYys2OYF"
   },
   "source": [
    "### *References*\n",
    "[1] https://arxiv.org/pdf/1506.02025.pdf <br>\n",
    "[2] https://en.wikipedia.org/wiki/Affine_transformation <br>\n",
    "[3] https://en.wikipedia.org/wiki/Transformation_matrix#Affine_transformations <br>\n",
    "[4] https://pytorch.org/docs/stable/nn.html#affine-grid <br>\n",
    "[5] https://pytorch.org/docs/stable/nn.html#torch.nn.functional.grid_sample <br>\n",
    "[6] http://cs231n.github.io/neural-networks-3/#anneal <br>\n",
    "[7] https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate <br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EEE4423_lab5_STN_problem.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
